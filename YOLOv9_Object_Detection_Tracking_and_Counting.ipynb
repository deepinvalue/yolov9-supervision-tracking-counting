{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q supervision=='0.18.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/WongKinYiu/yolov9.git -q\n",
    "!pip install -r yolov9/requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p yolov9/weights\n",
    "!wget -P yolov9/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'yolov9' not in sys.path:\n",
    "    sys.path.append('yolov9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML/DL\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# CV\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "# YOLOv9\n",
    "from models.common import DetectMultiBackend, AutoShape\n",
    "from utils.general import set_logging\n",
    "\n",
    "# Video Demonstration\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Supervision's `Detections` to Handle YOLOv9 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision import Detections as BaseDetections\n",
    "from supervision.config import CLASS_NAME_DATA_FIELD\n",
    "\n",
    "class ExtendedDetections(BaseDetections):\n",
    "    @classmethod\n",
    "    def from_yolov9(cls, yolov9_results) -> 'ExtendedDetections':\n",
    "        \"\"\"\n",
    "        Creates a Detections instance from YOLOv9 inference results.\n",
    "        \n",
    "        Args:\n",
    "            yolov9_results (yolov9.models.common.Detections):\n",
    "                The output Detections instance from YOLOv9.\n",
    "\n",
    "        Returns:\n",
    "            ExtendedDetections: A new Detections object that includes YOLOv9 detections.\n",
    "\n",
    "        Example:\n",
    "            results = model(image)\n",
    "            detections = ExtendedDetections.from_yolov9(results)\n",
    "        \"\"\"\n",
    "        xyxy, confidences, class_ids = [], [], []\n",
    "\n",
    "        for det in yolov9_results.pred:\n",
    "            for *xyxy_coords, conf, cls_id in reversed(det):\n",
    "                xyxy.append(torch.stack(xyxy_coords).cpu().numpy())\n",
    "                confidences.append(float(conf))\n",
    "                class_ids.append(int(cls_id))\n",
    "\n",
    "        class_names = np.array([yolov9_results.names[i] for i in class_ids])\n",
    "        \n",
    "        if not xyxy:\n",
    "            return cls.empty()  \n",
    "        \n",
    "        return cls(\n",
    "            xyxy=np.vstack(xyxy),\n",
    "            confidence=np.array(confidences),\n",
    "            class_id=np.array(class_ids),\n",
    "            data={CLASS_NAME_DATA_FIELD: class_names},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_logging(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = DetectMultiBackend(weights='yolov9/weights/yolov9-e.pt', device=device, data='yolov9/data/coco.yaml', fuse=True)\n",
    "model = AutoShape(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Set YOLOv9 Post-processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_yolov9(model, conf=0.2, iou=0.7, classes=None, agnostic_nms=False, max_det=1000):\n",
    "    model.conf = conf\n",
    "    model.iou = iou\n",
    "    model.classes = classes\n",
    "    model.agnostic = agnostic_nms\n",
    "    model.max_det = max_det\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Play Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(filename, width=500):\n",
    "    html = ''\n",
    "    video = open(filename,'rb').read()\n",
    "    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
    "    html += fr'<video width=500 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = \"../input/detection/test.mp4\"\n",
    "TARGET_VIDEO_PATH = \"output.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Specifying interested class indices from COCO dataset for detection.\n",
    "# Use specific indices for targeted detection or None to include all classes of COCO.\n",
    "# Example specific classes: 0 - person, 1 - bicycle, 2 - car, 3 - motorcycle, 5 - bus\n",
    "CLASSES = [0, 1, 2, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Object Detection with YOLOv9 and Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_video_info(model, config, source_path):\n",
    "    # Initialize and configure YOLOv9 model\n",
    "    model = prepare_yolov9(model, **config)\n",
    "    # Retrieve video information\n",
    "    video_info = sv.VideoInfo.from_video_path(source_path)\n",
    "    return model, video_info\n",
    "\n",
    "def setup_annotator():\n",
    "    # Initialize bounding box annotator\n",
    "    return sv.BoundingBoxAnnotator(thickness=2)\n",
    "\n",
    "def simple_annotate_frame(frame, model, annotator):\n",
    "    # Convert BGR to RGB\n",
    "    frame_rgb = frame[..., ::-1]\n",
    "    # Model prediction on single frame\n",
    "    results = model(frame_rgb, size=640, augment=False)\n",
    "    # Converting results to Supervision detections\n",
    "    detections = ExtendedDetections.from_yolov9(results)\n",
    "    # Annotate frame with bounding boxes\n",
    "    return annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "\n",
    "def simple_process_video(model, config=dict(conf=0.1, iou=0.45, classes=None,), source_path='input.mp4', target_path='output.mp4'):\n",
    "    model, _ = prepare_model_and_video_info(model, config, source_path)\n",
    "    annotator = setup_annotator()\n",
    "\n",
    "    def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "        return simple_annotate_frame(frame, model, annotator)\n",
    "    \n",
    "    # Process the whole video\n",
    "    sv.process_video(source_path=source_path, target_path=target_path, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_process_video(\n",
    "    model, \n",
    "    config=dict(conf=0.2, iou=0.6, classes=CLASSES), \n",
    "    source_path=SOURCE_VIDEO_PATH, \n",
    "    target_path='simple_object_detection_yolov9.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Detection, Tracking, and Counting with YOLOv9 and Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_video_info(model, config, source_path):\n",
    "    # Initialize and configure YOLOv9 model\n",
    "    model = prepare_yolov9(model, **config)\n",
    "    # Retrieve video information\n",
    "    video_info = sv.VideoInfo.from_video_path(source_path)\n",
    "    return model, video_info\n",
    "\n",
    "def create_byte_tracker(video_info):\n",
    "    # Setup BYTETracker with video information\n",
    "    return sv.ByteTrack(track_thresh=0.25, track_buffer=250, match_thresh=0.95, frame_rate=video_info.fps)\n",
    "\n",
    "def setup_annotators():\n",
    "    c = sv.ColorLookup.TRACK # Colorize based on the TRACK id, as opposed to INDEX or CLASS\n",
    "    # Initialize various annotators for bounding boxes, traces, and labels\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2, color_lookup=c)\n",
    "    round_box_annotator = sv.RoundBoxAnnotator(thickness=2, color_lookup=c)\n",
    "    corner_annotator = sv.BoxCornerAnnotator(thickness=2, color_lookup=c)\n",
    "    trace_annotator = sv.TraceAnnotator(thickness=2, trace_length=50, color_lookup=c)\n",
    "    label_annotator = sv.LabelAnnotator(text_scale=0.5, color_lookup=c)\n",
    "    return [bounding_box_annotator, round_box_annotator, corner_annotator], trace_annotator, label_annotator\n",
    "\n",
    "def setup_counting_zone(counting_zone, video_info):\n",
    "    # Configure counting zone based on provided parameters\n",
    "    max_width = video_info.width - 1\n",
    "    max_height = video_info.height - 1\n",
    "    if counting_zone == 'whole_frame':\n",
    "        polygon = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]])\n",
    "    else:\n",
    "        polygon = np.clip(counting_zone, a_min=[0, 0], a_max=[max_width, max_height])\n",
    "    polygon_zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=(video_info.width, video_info.height), triggering_position=sv.Position.CENTER)\n",
    "    polygon_zone_annotator = sv.PolygonZoneAnnotator(polygon_zone, sv.Color.ROBOFLOW, thickness=4*(2 if counting_zone=='whole_frame' else 1), text_thickness=2, text_scale=2)\n",
    "    return polygon_zone, polygon_zone_annotator\n",
    "\n",
    "def annotate_frame(frame, index, video_info, detections, byte_tracker, counting_zone, polygon_zone, polygon_zone_annotator, trace_annotator, annotators_list, label_annotator, show_labels, model):\n",
    "    # Apply tracking to detections\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Handle counting zone logic\n",
    "    if counting_zone is not None:\n",
    "        is_inside_polygon = polygon_zone.trigger(detections)\n",
    "        detections = detections[is_inside_polygon]\n",
    "        annotated_frame = polygon_zone_annotator.annotate(annotated_frame)\n",
    "    \n",
    "    # Annotate frame with traces\n",
    "    annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    \n",
    "    # Annotate frame with various bounding boxes\n",
    "    section_index = int(index / (video_info.total_frames / len(annotators_list)))\n",
    "    annotated_frame = annotators_list[section_index].annotate(scene=annotated_frame, detections=detections)\n",
    "    \n",
    "    # Optionally, add labels to the annotations\n",
    "    if show_labels:\n",
    "        annotated_frame = add_labels_to_frame(label_annotator, annotated_frame, detections, model)\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "def add_labels_to_frame(annotator, frame, detections, model):\n",
    "    labels = [f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\" for confidence, class_id, tracker_id in zip(detections.confidence, detections.class_id, detections.tracker_id)]\n",
    "    return annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "def process_video(model, config=dict(conf=0.1, iou=0.45, classes=None,), counting_zone=None, show_labels=False, source_path='input.mp4', target_path='output.mp4'):\n",
    "    model, video_info = setup_model_and_video_info(model, config, source_path)\n",
    "    byte_tracker = create_byte_tracker(video_info)\n",
    "    annotators_list, trace_annotator, label_annotator = setup_annotators()\n",
    "    polygon_zone, polygon_zone_annotator = setup_counting_zone(counting_zone, video_info) if counting_zone else (None, None)\n",
    "\n",
    "    def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "        frame_rgb = frame[..., ::-1]  # Convert BGR to RGB\n",
    "        results = model(frame_rgb, size=(video_info.height, video_info.width), augment=False)\n",
    "        detections = ExtendedDetections.from_yolov9(results)\n",
    "        return annotate_frame(frame, index, video_info, detections, byte_tracker, counting_zone, polygon_zone, polygon_zone_annotator, trace_annotator, annotators_list, label_annotator, show_labels, model)\n",
    "    \n",
    "    sv.process_video(source_path=source_path, target_path=target_path, callback=callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\n",
    "    model, \n",
    "    config=dict(conf=0.2, iou=0.6, classes=CLASSES), \n",
    "    counting_zone=None, # No counting\n",
    "    show_labels=False, \n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path='advanced_detection_tracking_yolov9.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection, Tracking, and Counting in Full Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\n",
    "    model, \n",
    "    config=dict(conf=0.2, iou=0.6, classes=CLASSES), \n",
    "    counting_zone='whole_frame', \n",
    "    show_labels=False, \n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path='full_frame_detection_tracking_counting_yolov9.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection, Tracking, and Counting in Selective Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\n",
    "    model, \n",
    "    config=dict(conf=0.2, iou=0.6, classes=CLASSES), \n",
    "    counting_zone=[[100, 50], [400, 50], [400, 250], [100, 250]], \n",
    "    show_labels=False, \n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path='selective_area_detection_tracking_counting_yolov9.mp4'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4495867,
     "sourceId": 7704563,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
