{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7701777,"sourceType":"datasetVersion","datasetId":4495867}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q supervision=='0.18.0'","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:20.572621Z","iopub.execute_input":"2024-02-26T07:32:20.572978Z","iopub.status.idle":"2024-02-26T07:32:34.381492Z","shell.execute_reply.started":"2024-02-26T07:32:20.572950Z","shell.execute_reply":"2024-02-26T07:32:34.380290Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone --recursive https://github.com/WongKinYiu/yolov9.git\n%cd yolov9/\n!pip install -r requirements.txt -q","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:34.383488Z","iopub.execute_input":"2024-02-26T07:32:34.383828Z","iopub.status.idle":"2024-02-26T07:32:49.283283Z","shell.execute_reply.started":"2024-02-26T07:32:34.383798Z","shell.execute_reply":"2024-02-26T07:32:49.282304Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'yolov9'...\nremote: Enumerating objects: 175, done.\u001b[K\nremote: Counting objects: 100% (48/48), done.\u001b[K\nremote: Compressing objects: 100% (31/31), done.\u001b[K\nremote: Total 175 (delta 32), reused 16 (delta 16), pack-reused 127\u001b[K\nReceiving objects: 100% (175/175), 618.93 KiB | 14.07 MiB/s, done.\nResolving deltas: 100% (72/72), done.\n/kaggle/working/yolov9\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir -p weights\n!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:49.284713Z","iopub.execute_input":"2024-02-26T07:32:49.285011Z","iopub.status.idle":"2024-02-26T07:32:52.040574Z","shell.execute_reply.started":"2024-02-26T07:32:49.284983Z","shell.execute_reply":"2024-02-26T07:32:52.039365Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# ML/DL\nimport numpy as np\nimport torch\n\n# CV\nimport cv2\nimport supervision as sv\n\n# YOLOv9\nfrom models.common import DetectMultiBackend, AutoShape\nfrom utils.torch_utils import select_device\nfrom utils.general import set_logging\n\n# Video Demonstration\nfrom IPython.display import HTML\nfrom base64 import b64encode","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:52.043113Z","iopub.execute_input":"2024-02-26T07:32:52.043427Z","iopub.status.idle":"2024-02-26T07:32:59.577093Z","shell.execute_reply.started":"2024-02-26T07:32:52.043399Z","shell.execute_reply":"2024-02-26T07:32:59.576314Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Extending Supervision's `Detections` to Handle YOLOv9 Results","metadata":{}},{"cell_type":"code","source":"from supervision import Detections as BaseDetections\nfrom supervision.config import CLASS_NAME_DATA_FIELD\n\nclass ExtendedDetections(BaseDetections):\n    @classmethod\n    def from_yolov9(cls, yolov9_results) -> 'ExtendedDetections':\n        \"\"\"\n        Creates a Detections instance from YOLOv9 inference results.\n        \n        Args:\n            yolov9_results (yolov9.models.common.Detections):\n                The output Detections instance from YOLOv9.\n\n        Returns:\n            ExtendedDetections: A new Detections object that includes YOLOv9 detections.\n\n        Example:\n            results = model(image)\n            detections = ExtendedDetections.from_yolov9(results)\n        \"\"\"\n        xyxy, confidences, class_ids = [], [], []\n\n        for det in yolov9_results.pred:\n            for *xyxy_coords, conf, cls_id in reversed(det):\n                xyxy.append(torch.stack(xyxy_coords).cpu().numpy())\n                confidences.append(float(conf))\n                class_ids.append(int(cls_id))\n\n        class_names = np.array([yolov9_results.names[i] for i in class_ids])\n        \n        if not xyxy:\n            return cls.empty()  \n        \n        return cls(\n            xyxy=np.vstack(xyxy),\n            confidence=np.array(confidences),\n            class_id=np.array(class_ids),\n            data={CLASS_NAME_DATA_FIELD: class_names},\n        )","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:59.578157Z","iopub.execute_input":"2024-02-26T07:32:59.578573Z","iopub.status.idle":"2024-02-26T07:32:59.587946Z","shell.execute_reply.started":"2024-02-26T07:32:59.578547Z","shell.execute_reply":"2024-02-26T07:32:59.587056Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Model","metadata":{}},{"cell_type":"code","source":"set_logging(verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:59.589265Z","iopub.execute_input":"2024-02-26T07:32:59.589631Z","iopub.status.idle":"2024-02-26T07:32:59.601853Z","shell.execute_reply.started":"2024-02-26T07:32:59.589599Z","shell.execute_reply":"2024-02-26T07:32:59.600915Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = select_device('0')\nmodel = DetectMultiBackend(weights='weights/yolov9-e.pt', device=device, data='data/coco.yaml', fuse=True)\nmodel = AutoShape(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:32:59.602960Z","iopub.execute_input":"2024-02-26T07:32:59.603198Z","iopub.status.idle":"2024-02-26T07:33:02.693622Z","shell.execute_reply.started":"2024-02-26T07:32:59.603177Z","shell.execute_reply":"2024-02-26T07:33:02.692584Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Helpers","metadata":{}},{"cell_type":"markdown","source":"## Function to Set YOLOv9 Post-processing Parameters","metadata":{}},{"cell_type":"code","source":"def prepare_yolov9(model, conf=0.2, iou=0.7, classes=None, agnostic_nms=False, max_det=1000):\n    model.conf = conf\n    model.iou = iou\n    model.classes = classes\n    model.agnostic = agnostic_nms\n    model.max_det = max_det\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:02.694917Z","iopub.execute_input":"2024-02-26T07:33:02.695265Z","iopub.status.idle":"2024-02-26T07:33:02.701075Z","shell.execute_reply.started":"2024-02-26T07:33:02.695233Z","shell.execute_reply":"2024-02-26T07:33:02.700109Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Function to Play Videos","metadata":{}},{"cell_type":"code","source":"def play(filename, width=500):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += fr'<video width=500 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:02.702141Z","iopub.execute_input":"2024-02-26T07:33:02.702393Z","iopub.status.idle":"2024-02-26T07:33:02.711879Z","shell.execute_reply.started":"2024-02-26T07:33:02.702372Z","shell.execute_reply":"2024-02-26T07:33:02.711131Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"SOURCE_VIDEO_PATH = \"../../input/detection/test.mp4\"\nTARGET_VIDEO_PATH = \"../output.mp4\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T07:33:02.715120Z","iopub.execute_input":"2024-02-26T07:33:02.715445Z","iopub.status.idle":"2024-02-26T07:33:02.722112Z","shell.execute_reply.started":"2024-02-26T07:33:02.715421Z","shell.execute_reply":"2024-02-26T07:33:02.721255Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Simple Object Detection with YOLOv9 and Supervision","metadata":{}},{"cell_type":"code","source":"def prepare_model_and_video_info(model, config, source_path):\n    # Initialize and configure YOLOv9 model\n    model = prepare_yolov9(model, **config)\n    # Retrieve video information\n    video_info = sv.VideoInfo.from_video_path(source_path)\n    return model, video_info\n\ndef setup_annotator():\n    # Initialize bounding box annotator\n    return sv.BoundingBoxAnnotator(thickness=2)\n\ndef simple_annotate_frame(frame, model, annotator):\n    # Convert BGR to RGB\n    frame_rgb = frame[..., ::-1]\n    # Model prediction on single frame\n    results = model(frame_rgb, size=640, augment=False)\n    # Converting results to Supervision detections\n    detections = ExtendedDetections.from_yolov9(results)\n    # Annotate frame with bounding boxes\n    return annotator.annotate(scene=frame.copy(), detections=detections)\n\ndef simple_process_video(model, config=dict(conf=0.1, iou=0.45, classes=None,), source_path=SOURCE_VIDEO_PATH, target_path=TARGET_VIDEO_PATH):\n    model, _ = prepare_model_and_video_info(model, config, source_path)\n    annotator = setup_annotator()\n\n    def callback(frame: np.ndarray, index: int) -> np.ndarray:\n        return simple_annotate_frame(frame, model, annotator)\n    \n    # Process the whole video\n    sv.process_video(source_path=source_path, target_path=target_path, callback=callback)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:02.723072Z","iopub.execute_input":"2024-02-26T07:33:02.723319Z","iopub.status.idle":"2024-02-26T07:33:02.733488Z","shell.execute_reply.started":"2024-02-26T07:33:02.723297Z","shell.execute_reply":"2024-02-26T07:33:02.732640Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"yolov9_config=dict(conf=0.3, iou=0.45, classes=None)\nsimple_process_video(model, config=yolov9_config, target_path='../simple_object_detection_yolov9.mp4');","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:02.734616Z","iopub.execute_input":"2024-02-26T07:33:02.734922Z","iopub.status.idle":"2024-02-26T07:33:30.487973Z","shell.execute_reply.started":"2024-02-26T07:33:02.734891Z","shell.execute_reply":"2024-02-26T07:33:30.487148Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Advanced Detection, Tracking, and Counting with YOLOv9 and Supervision","metadata":{}},{"cell_type":"code","source":"def setup_model_and_video_info(model, config, source_path):\n    # Initialize and configure YOLOv9 model\n    model = prepare_yolov9(model, **config)\n    # Retrieve video information\n    video_info = sv.VideoInfo.from_video_path(source_path)\n    return model, video_info\n\ndef create_byte_tracker(video_info):\n    # Setup BYTETracker with video information\n    return sv.ByteTrack(track_thresh=0.25, track_buffer=250, match_thresh=0.95, frame_rate=video_info.fps)\n\ndef setup_annotators():\n    # Initialize various annotators for bounding boxes, traces, and labels\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2, color_lookup=sv.ColorLookup.TRACK)\n    round_box_annotator = sv.RoundBoxAnnotator(thickness=2, color_lookup=sv.ColorLookup.TRACK)\n    corner_annotator = sv.BoxCornerAnnotator(thickness=2, color_lookup=sv.ColorLookup.TRACK)\n    trace_annotator = sv.TraceAnnotator(thickness=2, trace_length=50, color_lookup=sv.ColorLookup.TRACK)\n    label_annotator = sv.LabelAnnotator(text_scale=0.5, color_lookup=sv.ColorLookup.TRACK)\n    return [bounding_box_annotator, round_box_annotator, corner_annotator], trace_annotator, label_annotator\n\ndef setup_counting_zone(counting_zone, video_info):\n    # Configure counting zone based on provided parameters\n    if counting_zone == 'whole_frame':\n        polygon = np.array([[0, 0], [video_info.width-1, 0], [video_info.width-1, video_info.height-1], [0, video_info.height-1]])\n    else:\n        polygon = np.array(counting_zone)\n    polygon_zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=(video_info.width, video_info.height), triggering_position=sv.Position.CENTER)\n    polygon_zone_annotator = sv.PolygonZoneAnnotator(polygon_zone, sv.Color.ROBOFLOW, thickness=2*(2 if counting_zone=='whole_frame' else 1), text_thickness=1, text_scale=0.5)\n    return polygon_zone, polygon_zone_annotator\n\ndef annotate_frame(frame, index, video_info, detections, byte_tracker, counting_zone, polygon_zone, polygon_zone_annotator, trace_annotator, annotators_list, label_annotator, show_labels, model):\n    # Apply tracking to detections\n    detections = byte_tracker.update_with_detections(detections)\n    annotated_frame = frame.copy()\n    \n    # Handle counting zone logic\n    if counting_zone is not None:\n        is_inside_polygon = polygon_zone.trigger(detections)\n        detections = detections[is_inside_polygon]\n        annotated_frame = polygon_zone_annotator.annotate(annotated_frame)\n    \n    # Annotate frame with traces\n    annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n    \n    # Annotate frame with various bounding boxes\n    section_index = int(index / (video_info.total_frames / len(annotators_list)))\n    annotated_frame = annotators_list[section_index].annotate(scene=annotated_frame, detections=detections)\n    \n    # Optionally, add labels to the annotations\n    if show_labels:\n        annotated_frame = add_labels_to_frame(label_annotator, annotated_frame, detections, model)\n    \n    return annotated_frame\n\ndef add_labels_to_frame(annotator, frame, detections, model):\n    labels = [f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\" for confidence, class_id, tracker_id in zip(detections.confidence, detections.class_id, detections.tracker_id)]\n    return annotator.annotate(scene=frame, detections=detections, labels=labels)\n\ndef process_video(model, config=dict(conf=0.1, iou=0.45, classes=None,), counting_zone=None, show_labels=False, source_path=SOURCE_VIDEO_PATH, target_path=TARGET_VIDEO_PATH):\n    model, video_info = setup_model_and_video_info(model, config, source_path)\n    byte_tracker = create_byte_tracker(video_info)\n    annotators_list, trace_annotator, label_annotator = setup_annotators()\n    polygon_zone, polygon_zone_annotator = setup_counting_zone(counting_zone, video_info) if counting_zone else (None, None)\n\n    def callback(frame: np.ndarray, index: int) -> np.ndarray:\n        frame_rgb = frame[..., ::-1]  # Convert BGR to RGB\n        results = model(frame_rgb, size=608, augment=False)\n        detections = ExtendedDetections.from_yolov9(results)\n        return annotate_frame(frame, index, video_info, detections, byte_tracker, counting_zone, polygon_zone, polygon_zone_annotator, trace_annotator, annotators_list, label_annotator, show_labels, model)\n    \n    sv.process_video(source_path=source_path, target_path=target_path, callback=callback)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:30.489241Z","iopub.execute_input":"2024-02-26T07:33:30.489566Z","iopub.status.idle":"2024-02-26T07:33:30.511319Z","shell.execute_reply.started":"2024-02-26T07:33:30.489511Z","shell.execute_reply":"2024-02-26T07:33:30.510432Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Detection and Tracking","metadata":{}},{"cell_type":"code","source":"yolov9_config=dict(conf=0.3, iou=0.45, classes=[0, 2, 3])\nprocess_video(model, config=yolov9_config, counting_zone=None, show_labels=False, \n              target_path='../advanced_detection_tracking_yolov9.mp4');","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:30.512342Z","iopub.execute_input":"2024-02-26T07:33:30.512712Z","iopub.status.idle":"2024-02-26T07:33:57.452920Z","shell.execute_reply.started":"2024-02-26T07:33:30.512679Z","shell.execute_reply":"2024-02-26T07:33:57.451744Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Detection, Tracking, and Counting in Full Frame","metadata":{}},{"cell_type":"code","source":"yolov9_config=dict(conf=0.3, iou=0.45, classes=[0, 2, 3])\nprocess_video(model, config=yolov9_config, counting_zone='whole_frame', show_labels=False, \n              target_path='../full_frame_detection_tracking_counting_yolov9.mp4');","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:33:57.454543Z","iopub.execute_input":"2024-02-26T07:33:57.455176Z","iopub.status.idle":"2024-02-26T07:34:24.490224Z","shell.execute_reply.started":"2024-02-26T07:33:57.455133Z","shell.execute_reply":"2024-02-26T07:34:24.489334Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Detection, Tracking, and Counting in Selective Area","metadata":{}},{"cell_type":"code","source":"yolov9_config=dict(conf=0.3, iou=0.45, classes=[0, 2, 3])\ncounting_zone = [[250, 80], [380, 80], [380, 160], [250, 160]]\nprocess_video(model, config=yolov9_config, counting_zone=counting_zone, show_labels=True, \n              target_path='../selective_area_detection_tracking_counting_yolov9.mp4');","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:34:24.491569Z","iopub.execute_input":"2024-02-26T07:34:24.492226Z","iopub.status.idle":"2024-02-26T07:34:51.454992Z","shell.execute_reply.started":"2024-02-26T07:34:24.492188Z","shell.execute_reply":"2024-02-26T07:34:51.453831Z"},"trusted":true},"execution_count":16,"outputs":[]}]}